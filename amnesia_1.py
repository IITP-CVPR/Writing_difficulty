# -*- coding: utf-8 -*-
"""Amnesia_1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/105VRHZXRib5SefkaQ8dqmecsmOKikR8J
"""

from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet
from sklearn.ensemble import (
    RandomForestRegressor,
    GradientBoostingRegressor,
    AdaBoostRegressor,
    VotingRegressor
)
from sklearn.neural_network import MLPRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error, r2_score
import xgboost as xgb
from lightgbm import LGBMRegressor
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

# Load the newly uploaded attempts file and reattempt the process

# attempts = pd.read_excel("./attempts.xlsx")
df = pd.read_excel("/content/drive/MyDrive/full_data.xlsx")
df = df.dropna()
user_ids_list= df['WriterId']
# Drop 'WriterId' and extract features and target
X = df.drop(columns=['WriterId', 'Amnesia1', 'Amnesia_all'])
y = df['Amnesia1']

# Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Train-test split
#X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)
# === Split: 70 : 10 : 20 ===
X_train, X_temp, y_train, y_temp, id_train, id_temp = train_test_split(
    X_scaled, y, user_ids_list, test_size=0.30, random_state=42
)
X_val, X_test, y_val, y_test, id_val, id_test = train_test_split(
    X_temp, y_temp, id_temp, test_size=20/30, random_state=42
)

print(f"Train size: {len(X_train)}, Val size: {len(X_val)}, Test size: {len(X_test)}")

# Train a regression model
# reg_model =  RandomForestRegressor(n_estimators=100, random_state=42)
# reg_model.fit(X_train, y_train)

models = {
        'RandomForest': RandomForestRegressor(n_estimators=100, random_state=42),
        'XGBoost': xgb.XGBRegressor(random_state=42),
        'LinearRegression': LinearRegression(),
        'Lasso': Lasso(random_state=42),
        'Ridge': Ridge(random_state=42),
        'ElasticNet': ElasticNet(random_state=42),
        'MLP': MLPRegressor(random_state=42, max_iter=2000),
        'AdaBoost': AdaBoostRegressor(random_state=42),
        'KNN': KNeighborsRegressor(),
        'GradientBoosting': GradientBoostingRegressor(
            n_estimators=200,
            learning_rate=0.1,
            max_depth=3,
            random_state=42
        ),
        'LightGBM': LGBMRegressor(
            n_estimators=200,
            learning_rate=0.1,
            random_state=42
        ),
        'SVR': SVR(kernel='rbf', C=1.0, epsilon=0.1),
        'VotingEnsemble': VotingRegressor([
            ('gb', GradientBoostingRegressor(random_state=42)),
            ('rf', RandomForestRegressor(random_state=42)),
            ('xgb', xgb.XGBRegressor(random_state=42))
        ])
    }
    # Perform cross-validation for each model
print("\nPerforming testing accros all models...")
best_model=""; minm= 0;
model_score={}
for name, model in models.items():
    # Perform 5-fold cross-validation and calculate RMSE
    # rmse_scores = np.sqrt(-cv_scores)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    # amnesia_pred = y_test

    mae= mean_absolute_error(y_pred, y_test)
    mse= mean_squared_error(y_pred, y_test)
    rmse= np.sqrt(mse)
    r2 = r2_score(y_pred, y_test)
    model_score[name] = {
        'mae': mae,
        'mse': mse,
        'rmse': rmse,
        'r2': r2
    }
    if best_model==[]:
      amnesia_pred=y_pred
      best_model=model
    else:
      if minm<r2:
          minm=r2
          best_model=model

    print(f"{name}:")
    print(f"MAE: {mae:.4f}, MSE : {mse:.4f}, RMSE : {rmse:.4f}, R^2 : {r2:.4f}")

# Find the best model (lowest mean RMSE)
# best_model = min(cv_results.items(), key=lambda x: x[1]['mean_rmse'])
sorted_data = sorted(model_score.items(), key=lambda item: item[1]['r2'], reverse= True)
#print the results on decreasing order of r2 of all models:
for model_name, scores in sorted_data:
    print(f"{model_name}: MAE={scores['mae']:.4f}, MSE={scores['mse']:.4f}, RMSE : {scores['rmse']:.4f}, R^2={scores['r2']:.4f}")

print(f"\nBest performing model: {best_model}")
print(f"Best R^2: {minm}")

# Make predictions for all users
all_predictions = best_model.predict(X_scaled)

# Create results DataFrame
results_df = pd.DataFrame({
    'user_id': user_ids_list,
    'true_error_rate': y,
    'predicted_error_rate': all_predictions,
    'absolute_error': np.abs(y - all_predictions)
})

plot_error_rates(results_df)

from sklearn.linear_model import LinearRegression

for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    mae= mean_absolute_error(y_test, y_pred)
    mse= mean_squared_error(y_test, y_pred)
    rmse= np.sqrt(mse)
    r2 = r2_score(y_test, y_pred)
    mape = mean_absolute_percentage_error(y_test, y_pred)
    mad = np.median(np_abs(y_test - y_pred))

    model_score[name] = {
        'mae': mae,
        'mse': mse,
        'rmse': rmse,
        'r2': r2,
    }

    print(f"{name}:")
    print(f"MAE: {mae:.4f}, MSE : {mse:.4f}, RMSE : {rmse:.4f}, R^2 : {r2:.4f},  MAPE: {mape:.4f}, MAD: {mad:.4f}")

    # === Regression Scatter Plot ===

    idx_sample = np.random.choice(len(y_test), size=20, replace=False)
    y_true_sample = y_test.values[idx_sample]
    y_pred_sample = y_pred[idx_sample]

    # Fit regression line
    reg_line = LinearRegression()
    reg_line.fit(y_true_sample.reshape(-1, 1), y_pred_sample)
    line_x = np.linspace(y_test.min(), y_test.max(), 100)
    line_y = reg_line.predict(line_x.reshape(-1, 1))


    plt.figure(figsize=(10, 9))
    plt.scatter(y_true_sample, y_pred_sample, color="blue", s=150, alpha=0.8, label="")
    plt.plot([y_test.min(), y_test.max()],
             [y_test.min(), y_test.max()],
             linestyle="--", color="red", label="Theoretical diagonal")
    plt.plot(line_x, line_y, color="blue", linewidth=2, label="Regression fit")

    plt.xlabel("Actual", fontsize=30)
    plt.ylabel("Predicted", fontsize=30)
    #plt.title(f"Regression Plot (20 sample points) â€” {name}")
    plt.xlim(-0.25, 1)
    plt.ylim(-0.25, 1)
    plt.xticks(fontsize=30)
    plt.yticks(fontsize=30)
    plt.legend(fontsize=20)

    plt.tight_layout()
    plt.savefig(f"regression4_plots/{name}_scatter_regression.png", dpi=300)
    plt.show()